{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9357d12-9848-4808-9ee1-65d63e024d5c",
   "metadata": {},
   "source": [
    "# Generating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9b915-c22f-4268-bb15-34455db5c1ce",
   "metadata": {},
   "source": [
    "## Understanding Taxonomy\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The \"lab\" in InstructLab stands for Large-scale Alignment for Chat Bots.\n",
    "\n",
    "The LAB method is driven by taxonomies, which are largely created manually and with care.\n",
    "\n",
    "InstructLab crowdsources the process of tuning and improving models by collecting two types of data: knowledge and skills in a new open source community. These submissions are collected in a taxonomy of YAML files to be used in the synthetic data generation process. To help you understand the directory structure of a taxonomy, please refer to the following image.\n",
    "\n",
    "\n",
    "![redhat dog](./assets/taxonomy_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6823be-b9f2-444a-8e50-2bed36e073c6",
   "metadata": {},
   "source": [
    "## Add a new skill.\n",
    "\n",
    "The way the taxonomy approach works is that we provide a file, named qna.yaml, that contains a sample data set of questions and answers. This data set will be used in the process of creating many more synthetic data examples.  The important thing to understand about the qna.yaml file is that it must follow a specific schema for InstructLab to use to synthetically generate more examples. \n",
    "\n",
    "Instead of having to type this information in by hand, simply run the following command to copy the qna.yaml file to your taxonomy directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee012c67-7605-4d1f-89ca-3ecedd614814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ea875-ca37-41ae-8ee6-2c57947b3b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp ./assets/qna.yaml ./taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables/qna.yaml \n",
    "!head ./taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables/qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c1845-e41e-4a5d-9e92-f1b28ee432d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp ./assets/attribution.txt ./taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables/attribution.txt \n",
    "!head ./taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables/attribution.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df0634c-be19-485e-ae6b-881d5b77f982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head ~/instructlab/taxonomy/compositional_skills/extraction/inference/quantitative/asciidoc/tables/qna.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918c96d-a3b9-41b1-ae90-4901490c7511",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "InstructLab allows you to validate your taxonomy files before generating additional data. You can accomplish this by using the `ilab diff` command as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07281a-85c8-47cf-a5c3-290022a1989b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ilab diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c09db-6a27-460a-badb-c581dd51fca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate synthetic data\n",
    "\n",
    "Okay, so far so good. Now, letâ€™s move on to the AWESOME part. We are going to use our taxonomy, which contains our qna.yaml file, to have the LLM automatically generate more examples. The generate step can often take a while and is dependent on the number of instructions that you want to generate. What this means is that InstructLab will generate X number of additional questions and answers based on the samples provided. To give you an idea of how long this takes, generating 100 additional questions and answers typically takes about 7 minutes when using a nicely specced consumer-grade GPU-accelerated Linux machine. This can take around 20 minutes using Apple Silicon and depends on many factors. For the purpose of this workshop, we are only going to generate 5 additional samples. To do this, you would issue the following command:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8df577-2c28-4000-ac9e-ab4aeb4c2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ilab generate --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f82b0-6f8c-4043-b846-69e041d15ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ilab generate --model granite-7b-lab-Q4_K_M --num-instructions 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f03b9-ff87-47fc-84ed-c921c26e76ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
